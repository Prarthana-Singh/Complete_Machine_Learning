
---

##  **A. Basic Questions (Conceptual Understanding)**

1. What are evaluation metrics in machine learning?
2. Why are evaluation metrics important?
3. What are the different types of evaluation metrics?
4. What is the difference between **training accuracy** and **testing accuracy**?
5. Why accuracy alone is not always a good metric?
6. What are the common evaluation metrics used for **classification problems**?
7. What are the common evaluation metrics used for **regression problems**?
8. What is the difference between **evaluation** and **validation**?
9. Why do we use different metrics for different models?
10. How do you choose the right evaluation metric for a given problem?

---

##  **B. Classification Metrics (Binary & Multiclass)**

11. What is a **Confusion Matrix**? Explain its components (TP, TN, FP, FN).
12. How do you interpret a confusion matrix?
13. What is **Accuracy** and how is it calculated?
14. Why can accuracy be misleading in imbalanced datasets?
15. What is **Precision** and when is it important?
16. What is **Recall (Sensitivity)** and when is it important?
17. Explain **Precision-Recall Tradeoff**.
18. What is **F1 Score** and why is it used?
19. How do you calculate the F1 score?
20. What is **Weighted F1 Score** and how is it different from Macro F1?
21. What is **Specificity (True Negative Rate)**?
22. What is **Balanced Accuracy** and when is it useful?
23. Explain the **ROC Curve**.
24. What does **AUC (Area Under the Curve)** represent?
25. What does a high AUC value indicate?
26. What is the **Precision-Recall Curve**?
27. What is the difference between **ROC-AUC** and **PR-AUC**?
28. What is **Log Loss** and where is it used?
29. What is **Hamming Loss**?
30. How do you handle **multi-class** classification metrics?
31. What are **micro**, **macro**, and **weighted averages** in multi-class evaluation?
32. How do you interpret a **classification report** in scikit-learn?
33. What is **Matthews Correlation Coefficient (MCC)**?
34. What is **Cohen’s Kappa Score**?
35. When would you use **Top-K Accuracy**?

---

##  **C. Regression Metrics**

36. What metrics are used for regression tasks?
37. What is **Mean Absolute Error (MAE)**?
38. What is **Mean Squared Error (MSE)**?
39. What is **Root Mean Squared Error (RMSE)**?
40. When would you prefer MAE over MSE?
41. What does **R² (Coefficient of Determination)** indicate?
42. What does an R² of 0.85 mean?
43. What is **Adjusted R²** and how is it different from R²?
44. What is **Mean Absolute Percentage Error (MAPE)**?
45. What is the drawback of MAPE?
46. What is **Mean Bias Deviation (MBD)**?
47. What is **Explained Variance Score**?
48. What are **robust regression metrics** for datasets with outliers?
49. How do you interpret RMSE in real-world terms?
50. Why should regression metrics be compared on the same dataset only?

---

##  **D. Ranking & Probabilistic Metrics**

51. What is **Log-Likelihood** in model evaluation?
52. What is **Brier Score**?
53. What is **Cross-Entropy Loss**?
54. What is **NDCG (Normalized Discounted Cumulative Gain)**?
55. What is **Mean Average Precision (mAP)**?
56. What is **Hit Rate** in recommendation systems?
57. What is **Mean Reciprocal Rank (MRR)**?
58. What are **ranking-based evaluation metrics** used for?
59. Explain how **recommendation system metrics** differ from classification metrics.

---

##  **E. Handling Imbalanced Data**

60. What metric would you use for imbalanced data?
61. Why is accuracy not suitable for imbalanced datasets?
62. How can you use **Precision, Recall, and F1 score** for imbalanced data?
63. What is **ROC-AUC’s limitation** with imbalanced datasets?
64. Why is **PR-AUC** often preferred for imbalanced data?
65. How would you evaluate a **fraud detection model**?
66. How do **SMOTE and other sampling techniques** impact evaluation metrics?
67. What are **cost-sensitive metrics**?

---

##  **F. Model Comparison & Interpretation**

68. How do you compare two models using evaluation metrics?
69. What is **cross-validation score** and how does it relate to evaluation?
70. What’s the difference between **training performance** and **validation performance**?
71. What is **model calibration** and how does it affect evaluation?
72. How can you visualize evaluation metrics?
73. What is the **trade-off between bias and variance** in metrics?
74. How can you detect **overfitting** using evaluation metrics?
75. What is the role of evaluation metrics in **hyperparameter tuning**?

---

##  **G. Practical / Scenario-Based Questions**

76. You built a model with 99% accuracy, but the client says it’s useless — what could be wrong?
77. Your classification model has high precision but low recall — what does it mean?
78. How would you choose metrics for a **credit card fraud detection model**?
79. What metric would you use for **spam email detection**?
80. What metric would you use for a **recommendation system**?
81. How would you evaluate a **time-series forecasting model**?
82. Which metrics would you choose for a **ranking problem** like search engines?
83. Which metric would you use to measure **customer churn prediction**?
84. What is the best metric for **imbalanced binary classification**?
85. If your regression model has an R² of 0.2 but low RMSE, how would you interpret it?

---

##  **H. Advanced & Tricky Questions**

86. Can you have a high AUC but low accuracy?
87. Can precision and recall both increase simultaneously?
88. Can RMSE be negative?
89. Can R² be negative?
90. Why is the **F1 score** the harmonic mean, not the arithmetic mean?
91. Is a higher R² always better?
92. How do you evaluate an **unsupervised learning model**?
93. How do you handle **custom evaluation metrics** in scikit-learn?
94. What is the difference between **metric optimization** and **loss optimization**?
95. How can evaluation metrics help in **feature selection**?
96. How do **cross-validation metrics** differ from single-split metrics?
97. What happens if your evaluation metric is not aligned with your business goal?
98. How do you evaluate **regression residuals**?
99. What is **Model Drift** and how do you track it using evaluation metrics?
100. How would you explain evaluation metrics to a non-technical stakeholder?

---
