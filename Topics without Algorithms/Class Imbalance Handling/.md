
---

##  **A. Basic Conceptual Questions**

1. What is class imbalance in a dataset?
2. Why is class imbalance a problem in machine learning?
3. How does class imbalance affect model performance?
4. Can accuracy be trusted on imbalanced datasets? Why or why not?
5. What types of problems commonly have class imbalance?
6. How does class imbalance affect metrics like precision, recall, and F1-score?
7. How does class imbalance affect model bias and variance?
8. What is the difference between **minority class** and **majority class**?
9. How do you identify class imbalance in a dataset?
10. What is the difference between **binary** and **multi-class class imbalance**?

---

##  **B. Evaluation Metrics for Imbalanced Data**

11. Why is accuracy not a good metric for imbalanced datasets?
12. What metrics are preferred for imbalanced datasets?
13. How do precision, recall, and F1-score help in imbalanced problems?
14. What is **ROC-AUC**, and how is it used for imbalanced datasets?
15. What is **PR-AUC**, and why is it often preferred over ROC-AUC?
16. What is the **G-mean**, and how does it evaluate imbalanced datasets?
17. How does Matthews Correlation Coefficient (MCC) help?
18. Can confusion matrix analysis help with class imbalance? How?
19. What is the difference between micro-average and macro-average metrics?
20. How do you choose the right metric depending on the business problem?

---

##  **C. Techniques to Handle Class Imbalance**

21. What is **undersampling**?
22. What is **oversampling**?
23. What is **SMOTE (Synthetic Minority Oversampling Technique)**?
24. What are variants of SMOTE, like **Borderline-SMOTE** or **ADASYN**?
25. What is **Tomek Links** and how is it used?
26. What is **Cluster Centroids** for imbalance handling?
27. What is **NearMiss** undersampling?
28. What is **random oversampling** and its drawbacks?
29. What is **class weighting** and how does it work?
30. How do you apply **cost-sensitive learning**?
31. How does **ensemble learning** help with class imbalance?
32. What is **Balanced Random Forest**?
33. What is **EasyEnsemble** and **BalanceCascade**?
34. How do you decide between oversampling and undersampling?
35. How do you combine multiple techniques for better results?

---

##  **D. Scenario-Based / Practical Questions**

36. You have a fraud detection dataset with 1% fraud cases — what approach would you take?
37. Your spam detection model always predicts the majority class — how do you fix it?
38. Your recall is low for the minority class — what steps would you take?
39. How do you tune thresholds to improve performance on imbalanced data?
40. In a medical diagnosis problem, which is more critical — FP or FN?
41. How would you handle imbalance in multi-class problems?
42. You applied SMOTE but performance didn’t improve — why?
43. Your model performs well on the majority class but poorly on the minority class — what’s your plan?
44. How do you explain class imbalance handling to a non-technical stakeholder?
45. How does feature selection impact class imbalance solutions?

---

##  **E. Advanced / Theoretical Questions**

46. What are the limitations of oversampling and undersampling?
47. How does SMOTE generate synthetic samples?
48. How do you avoid overfitting when using SMOTE?
49. How do ensemble methods reduce class imbalance effects?
50. How does class weighting interact with loss functions?
51. Can imbalance handling techniques introduce bias?
52. How do you handle imbalance in **deep learning models**?
53. How is imbalance handled in **multi-label classification**?
54. How does imbalance affect decision trees and SVMs differently?
55. How does cross-validation interact with imbalance handling techniques?

---

##  **F. Python / Implementation Questions**

56. How do you implement **SMOTE** in Python using `imbalanced-learn`?
57. How do you perform **random oversampling/undersampling** in Python?
58. How do you apply **class weights** in scikit-learn models?
59. How do you evaluate model performance after applying imbalance handling?
60. How do you combine **SMOTE with cross-validation**?
61. How do you implement **Balanced Random Forest** in Python?
62. How do you use **ADASYN** for generating synthetic samples?
63. How do you visualize class imbalance before and after sampling?
64. How do you check for overfitting after applying oversampling?
65. How do you implement threshold tuning to improve recall for the minority class?

---

##  **G. Trick / Deep Understanding Questions**

66. Can oversampling introduce noise in the dataset?
67. Can undersampling remove useful information?
68. Can SMOTE cause overlapping between classes?
69. How do you choose the right percentage for oversampling?
70. Can you apply oversampling to the test set? Why or why not?
71. Why is it important to apply imbalance handling **only on the training set**?
72. Can class imbalance be solved by simply collecting more data?
73. How do you deal with extreme imbalance, e.g., 0.1% minority class?
74. How does imbalance affect probabilistic models?
75. How do you combine multiple metrics to evaluate imbalanced data?

---

##  **H. Business / Application-Oriented Questions**

76. How do you evaluate a fraud detection model with 0.5% fraud cases?
77. How do you evaluate a rare disease prediction model?
78. How do you explain FP/FN impact to business stakeholders?
79. How would you prioritize minimizing FP or FN depending on the application?
80. How does class imbalance affect decision-making in marketing, finance, or healthcare?
81. How do you justify using SMOTE or class weighting in a production model?
82. How do you measure ROI after improving minority class prediction?
83. How would you handle class imbalance in churn prediction?
84. How would you deal with an imbalanced dataset in an online recommendation system?
85. How do you maintain fairness while handling imbalance in sensitive applications?

---
